<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>ml_pipeline_demo - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201605101153170700-ecbec5879e34c7cbc28283a2045353a80cbf92c8/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201605101153170700-ecbec5879e34c7cbc28283a2045353a80cbf92c8/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201605101153170700-ecbec5879e34c7cbc28283a2045353a80cbf92c8/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201605101153170700-ecbec5879e34c7cbc28283a2045353a80cbf92c8/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/201605101153170700-ecbec5879e34c7cbc28283a2045353a80cbf92c8/img/favicon.ico"/>
<script>window.settings = {"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","workspaceFeaturedLinks":[{"linkURI":"https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html","displayName":"Databricks Guide","icon":"question"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/sample_applications/index.html","displayName":"Application Examples","icon":"code"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/courses/index.html","displayName":"Training","icon":"graduation-cap"}],"dbcForumURL":"http://forums.databricks.com/","nodeInfo":{"node_types":[{"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":3.0,"node_type_id":"dev-tier-node","description":"Community Optimized","container_memory_mb":6000,"memory_mb":6144,"num_cores":0.88}],"default_node_type_id":"dev-tier-node"},"enableThirdPartyApplicationsUI":false,"enableClusterAcls":false,"notebookRevisionVisibilityHorizon":6,"enableTableHandler":true,"isAdmin":true,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-west-2c","isDefault":true},{"id":"us-west-2b","isDefault":false},{"id":"us-west-2a","isDefault":false}],"enablePublishNotebooks":true,"enableJobAclsConfig":false,"enableFullTextSearch":false,"enableElasticSparkUI":false,"clusters":true,"allowRunOnPendingClusters":true,"applications":false,"fileStoreBase":"FileStore","configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableResetPassword":true,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-1.6.1-hadoop1-jenkins-ip-10-30-10-144-Uecbec5879e-S2368283920-2016-05-10-19:12:50.761403","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-1.4-jenkins-ip-10-30-10-144-Uecbec5879e-S9c254ab12a-2016-05-10-19:12:50.761403","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"master","displayName":"Spark master (dev)","packageLabel":"","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-1.6.1-hadoop1-jenkins-ip-10-30-10-144-Uecbec5879e-S2368283920-2016-05-10-19:12:50.761403","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-1.6.1-hadoop1-jenkins-ip-10-30-10-144-Uecbec5879e-S2368283920-2016-05-10-19:12:50.761403","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-1.6.1-hadoop2-jenkins-ip-10-30-10-144-Uecbec5879e-S2368283920-2016-05-10-19:12:50.761403","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-1.5-jenkins-ip-10-30-10-144-Uecbec5879e-S9ca52d000d-2016-05-10-19:12:50.761403","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-1.3-jenkins-ip-10-30-10-144-Uecbec5879e-Sa2ee4664b2-2016-05-10-19:12:50.761403","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (branch preview)","packageLabel":"spark-image-9a9663cc79bbcb0874f187d159c1f7fb78ab750b3ef14472f179325814f22443","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-1.6.0-jenkins-ip-10-30-10-144-Uecbec5879e-Sf90f83597b-2016-05-10-19:12:50.761403","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-1.6.1-hadoop2-jenkins-ip-10-30-10-144-Uecbec5879e-S2368283920-2016-05-10-19:12:50.761403","upgradable":true,"deprecated":false,"customerVisible":false}],"enableRestrictedClusterCreation":true,"enableFeedback":true,"enableClusterAutoScaling":false,"defaultNumWorkers":0,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"driverStdoutFilePrefix":"stdout","enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableNotebookHistoryDiffing":true,"branch":"2.19.1","accountsLimit":3,"enableNotebookGitBranching":true,"local":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":6,"deploymentMode":"production","useSpotForWorkers":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"notifyLastLogin":false,"enableNotebookGitVersioning":true,"files":"files/","enableDriverLogsUI":true,"disableLegacyDashboards":true,"enableWorkspaceAclsConfig":false,"dropzoneMaxFileSize":4096,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":false,"enableMavenLibraries":true,"displayRowLimit":1000,"defaultSparkVersion":{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-1.6.1-hadoop1-jenkins-ip-10-30-10-144-Uecbec5879e-S2368283920-2016-05-10-19:12:50.761403","upgradable":true,"deprecated":false,"customerVisible":true},"enableMountAclsConfig":false,"enableClusterAclsByTier":false,"disallowAddingAdmins":true,"enableSparkConfUI":true,"featureTier":"DEVELOPER_BASIC_TIER","enableOrgSwitcherUI":true,"clustersLimit":1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":false,"enableClusterDeltaUpdates":true,"useFixedStaticNotebookVersionForDevelopment":false,"enableMountAcls":false,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"mailto:feedback@databricks.com","enableMountAclService":true,"enableWorkspaceAclService":true,"enableWorkspaceAcls":false,"gitHash":"ecbec5879e34c7cbc28283a2045353a80cbf92c8","showWorkspaceFeaturedLinks":true,"allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":true,"showDevTierBetaVersion":true,"enableDebugUI":false,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/201605101153170700-ecbec5879e34c7cbc28283a2045353a80cbf92c8/","enableSparkPackages":true,"dynamicSparkVersions":true,"enableNotebookHistoryUI":true,"showDebugCounters":false,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/1%20QuickStart%20Notebooks%20(Python).html","displayName":"Getting Started","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/3%20QuickStart%20DataFrames%20(Scala).html","displayName":"Introduction to DataFrames","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.cloud.databricks.com/docs/latest/featured_notebooks/DecisionTrees-Example.html","displayName":"Decision Trees in Spark","icon":"img/home/Scala_icon.svg"}],"upgradeURL":"https://accounts.cloud.databricks.com/registration.html","notebookLoadingBackground":"#fff","enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableTerminal":false,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"accounts":true,"useFramedStaticNotebooks":true,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":3213500062218094,"name":"ml_pipeline_demo","language":"scala","commands":[{"version":"CommandV1","origId":3213500062218097,"guid":"4985cb89-aa99-45e7-9fc0-ebe4486de179","subtype":"command","commandType":"auto","position":2.0,"command":"%md **Example**\n\nIn this example, we create a simple ML Pipeline, wrapped by a Cross-Validator.  We will save both the workflow (before fitting) and the fitted model.\n\nOur Pipeline has this nested structure:\n* `CrossValidator`\n  * `Pipeline`\n    * `Tokenizer`\n    * `HashingTF`\n    * `LogisticRegression`","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":null,"showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"f33b195e-ab5f-4c34-873a-e75b578d94fe"},{"version":"CommandV1","origId":3213500062218098,"guid":"7be2e082-d218-4e9e-a665-6aa4015ee0ae","subtype":"command","commandType":"auto","position":2.5,"command":"import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nimport org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nimport org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:3: error: '}' expected but ';' found.\n       import org.apache.spark.ml.classification.LogisticRegression\n^\n</div>","error":null,"workflows":[],"startTime":1.463091428812E12,"submitTime":1.463091421687E12,"finishTime":1.463091430372E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":null,"showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"c15cc53d-cf14-4887-8a0e-c724f84bb799"},{"version":"CommandV1","origId":3213500062218099,"guid":"8648b28a-ebb4-46e6-b8bb-6bb6e4dc1ea9","subtype":"command","commandType":"auto","position":3.0,"command":"// Create the workflow\"\nval tokenizer = new Tokenizer()\n  .setInputCol(\"text\")\n  .setOutputCol(\"words\")\n\nval hashingTF = new HashingTF()\n  .setInputCol(tokenizer.getOutputCol)\n  .setOutputCol(\"features\")\n\nval lr = new LogisticRegression()\n  .setMaxIter(3)\n\nval pipeline = new Pipeline()\n  .setStages(Array(tokenizer, hashingTF, lr))\n\n///////////////\nval paramMaps = new ParamGridBuilder()\n  .addGrid(hashingTF.numFeatures, Array(10, 20))\n  .addGrid(lr.regParam, Array(0.0, 0.1))\n  .build()\n// Note: In practice, you would likely want to use more features and more iterations for LogisticRegression.\n\nval evaluator = new BinaryClassificationEvaluator()\n\nval cv = new CrossValidator()\n  .setEstimator(pipeline)\n  .setEvaluator(evaluator)\n  .setNumFolds(2)\n  .setEstimatorParamMaps(paramMaps)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_a76e8bb8d367\nhashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_ce1fcd01677e\nlr: org.apache.spark.ml.classification.LogisticRegression = logreg_a897c2f1d8c4\npipeline: org.apache.spark.ml.Pipeline = pipeline_5d3bf85f6041\nparamMaps: Array[org.apache.spark.ml.param.ParamMap] = \nArray({\n\thashingTF_ce1fcd01677e-numFeatures: 10,\n\tlogreg_a897c2f1d8c4-regParam: 0.0\n}, {\n\thashingTF_ce1fcd01677e-numFeatures: 20,\n\tlogreg_a897c2f1d8c4-regParam: 0.0\n}, {\n\thashingTF_ce1fcd01677e-numFeatures: 10,\n\tlogreg_a897c2f1d8c4-regParam: 0.1\n}, {\n\thashingTF_ce1fcd01677e-numFeatures: 20,\n\tlogreg_a897c2f1d8c4-regParam: 0.1\n})\nevaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_efc451047dc9\ncv: org.apache.spark.ml.tuning.CrossValidator = cv_8b5bbd1a5b3d\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:30: error: not found: type HashingTF\n       val hashingTF = new HashingTF()\n                           ^\n&lt;console&gt;:33: error: not found: type LogisticRegression\n       val lr = new LogisticRegression()\n                    ^\n&lt;console&gt;:35: error: not found: type Pipeline\n       val pipeline = new Pipeline()\n                          ^\n&lt;console&gt;:38: error: not found: type ParamGridBuilder\n       val paramMaps = new ParamGridBuilder()\n                           ^\n&lt;console&gt;:43: error: not found: type BinaryClassificationEvaluator\n       val evaluator = new BinaryClassificationEvaluator()\n                           ^\n&lt;console&gt;:45: error: not found: type CrossValidator\n       val cv = new CrossValidator()\n                    ^\n</div>","error":null,"workflows":[],"startTime":1.463091433416E12,"submitTime":1.463091426291E12,"finishTime":1.463091433822E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":null,"showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"a0e4cce9-8988-4ed9-a2de-bc64b04fe489"},{"version":"CommandV1","origId":3213500062218100,"guid":"5661cdc1-e3e4-42cf-be73-ffbea23ffe49","subtype":"command","commandType":"auto","position":4.0,"command":"// Save the workflow\ncv.write.overwrite().save(\"/tmp/pipeline\")\n// You can also write this, following the spark.mllib API (but this will not overwrite):\n//  cv.save(\"/home/spark/1.6/ml/savedPipeline\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:49: error: too many arguments for method overwrite: ()org.apache.spark.ml.util.MLWriter\n              cv.write.overwrite(true).save(&quot;/home/spark/1.6/ml/savedPipeline&quot;)\n                                ^\n</div>","error":null,"workflows":[],"startTime":1.463091437073E12,"submitTime":1.463091429949E12,"finishTime":1.463091445068E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":null,"showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"a02a1ca2-7a6a-4737-88f4-e1abac90f0a7"},{"version":"CommandV1","origId":3213500062218101,"guid":"a4098a5e-ecde-48e0-9448-92944c228eff","subtype":"command","commandType":"auto","position":4.40625,"command":"// Load the workflow back\nval sameCV = CrossValidator.load(\"/tmp/pipeline\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">sameCV: org.apache.spark.ml.tuning.CrossValidator = cv_8b5bbd1a5b3d\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.463091449579E12,"submitTime":1.463091442454E12,"finishTime":1.463091452485E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":null,"showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"db1b7106-3765-4f52-a6f8-cc0cca51ff6e"},{"version":"CommandV1","origId":3213500062218102,"guid":"2425ce03-9888-43b1-92c0-d927885e0f23","subtype":"command","commandType":"auto","position":4.609375,"command":"%md The loaded workflow is exactly the same: same transformers, parameters, etc.\n\nWe next fit the workflow to produce a model, which we can save and load as well.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":null,"showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"c3b29935-39b7-41df-8c55-2754d8bf2ad2"},{"version":"CommandV1","origId":3213500062218103,"guid":"6c3de062-3d27-48c5-9831-5cf222f23ec9","subtype":"command","commandType":"auto","position":4.8125,"command":"val data = Seq(\n  (\"Hello hello world\", 0.0),\n  (\"Hello how are you world\", 0.0),\n  (\"What is the world\", 1.0),\n  (\"There was a plant in the water\", 0.0),\n  (\"Water was around the world\", 1.0),\n  (\"And then hello again\", 0.0))\nval df = sqlContext.createDataFrame(data).toDF(\"text\", \"label\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">data: Seq[(String, Double)] = List((Hello hello world,0.0), (Hello how are you world,0.0), (What is the world,1.0), (There was a plant in the water,0.0), (Water was around the world,1.0), (And then hello again,0.0))\ndf: org.apache.spark.sql.DataFrame = [text: string, label: double]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.463091455589E12,"submitTime":1.463091448464E12,"finishTime":1.463091455923E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":null,"showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"e503acaa-e98b-4e59-9e45-4eb879dccd1a"},{"version":"CommandV1","origId":3213500062218104,"guid":"6376aef7-be7f-4efa-bbf5-08f3117eb883","subtype":"command","commandType":"auto","position":5.0,"command":"// Fit the model\nval cvModel = cv.fit(df)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">cvModel: org.apache.spark.ml.tuning.CrossValidatorModel = cv_8b5bbd1a5b3d\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"java.lang.IllegalArgumentException: requirement failed: Column label must be of type DoubleType but was actually IntegerType.","error":"<div class=\"ansiout\">\tat scala.Predef$.require(Predef.scala:233)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)\n\tat org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:53)\n\tat org.apache.spark.ml.classification.Classifier.org$apache$spark$ml$classification$ClassifierParams$$super$validateAndTransformSchema(Classifier.scala:56)\n\tat org.apache.spark.ml.classification.ClassifierParams$class.validateAndTransformSchema(Classifier.scala:40)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifier.org$apache$spark$ml$classification$ProbabilisticClassifierParams$$super$validateAndTransformSchema(ProbabilisticClassifier.scala:53)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifierParams$class.validateAndTransformSchema(ProbabilisticClassifier.scala:37)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifier.validateAndTransformSchema(ProbabilisticClassifier.scala:53)\n\tat org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:116)\n\tat org.apache.spark.ml.Pipeline$$anonfun$transformSchema$4.apply(Pipeline.scala:173)\n\tat org.apache.spark.ml.Pipeline$$anonfun$transformSchema$4.apply(Pipeline.scala:173)\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51)\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108)\n\tat org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:173)\n\tat org.apache.spark.ml.tuning.CrossValidator.transformSchema(CrossValidator.scala:129)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:68)\n\tat org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:91)</div>","workflows":[],"startTime":1.463091458229E12,"submitTime":1.463091451103E12,"finishTime":1.463091461466E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":null,"showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"9d34c453-de0a-4d6d-932b-82af148ddc19"},{"version":"CommandV1","origId":3213500062218105,"guid":"de1db5ec-fcc6-4882-816e-a23f2d7e49dc","subtype":"command","commandType":"auto","position":6.0,"command":"// Save the fitted model\ncvModel.write.overwrite().save(\"/tmp/fittedPipeline\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.463091465065E12,"submitTime":1.46309145794E12,"finishTime":1.463091479915E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":null,"showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"cf062a4c-2fea-458a-a884-9791bc5d2b9d"},{"version":"CommandV1","origId":3213500062218106,"guid":"18a8412e-a258-40e0-924c-952a0fe85361","subtype":"command","commandType":"auto","position":7.0,"command":"// Load the fitted model back\nval sameCVModel = CrossValidatorModel.load(\"/tmp/fittedPipeline\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">sameCVModel: org.apache.spark.ml.tuning.CrossValidatorModel = cv_8b5bbd1a5b3d\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<div class=\"ansiout\">&lt;console&gt;:41: error: not found: value CrossValidatorModel\n       val sameCVModel = CrossValidatorModel.load(&quot;/home/spark/1.6/ml/fittedPipeline&quot;)\n                         ^\n</div>","error":null,"workflows":[],"startTime":1.463091504531E12,"submitTime":1.463091497408E12,"finishTime":1.463091511341E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":null,"showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"nuid":"533641c6-7a6e-40d4-bfa2-2cbc26d34fce"}],"dashboards":[],"guid":"c3450742-b09b-4aed-b4ee-c19e8478214d","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/201605101153170700-ecbec5879e34c7cbc28283a2045353a80cbf92c8/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":-1,"guid":"214f4f71-8ca6-469b-9fbb-da9433af2ae1","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/201605101153170700-ecbec5879e34c7cbc28283a2045353a80cbf92c8/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>